{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":157908870,"sourceType":"kernelVersion"}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/ryanzhumich/AESLC","metadata":{"_uuid":"500ff78d-d7d6-418e-ad99-6735cf9908ec","_cell_guid":"4694db26-6ffb-4bdd-83f4-c756fcd21e53","collapsed":false,"id":"X3T7N7-auuQ1","executionInfo":{"status":"ok","timestamp":1701624873016,"user_tz":-330,"elapsed":27194,"user":{"displayName":"Adarsh J","userId":"16994502924081621860"}},"outputId":"c4aacc01-e014-461d-84dd-25dc32c8495a","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-06T12:24:43.555829Z","iopub.execute_input":"2024-01-06T12:24:43.556368Z","iopub.status.idle":"2024-01-06T12:24:45.670039Z","shell.execute_reply.started":"2024-01-06T12:24:43.556321Z","shell.execute_reply":"2024-01-06T12:24:45.669235Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'AESLC'...\nremote: Enumerating objects: 17469, done.\u001b[K\nremote: Counting objects: 100% (8/8), done.\u001b[K\nremote: Compressing objects: 100% (8/8), done.\u001b[K\nremote: Total 17469 (delta 1), reused 0 (delta 0), pack-reused 17461\u001b[K\nReceiving objects: 100% (17469/17469), 7.36 MiB | 23.05 MiB/s, done.\nResolving deltas: 100% (48/48), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"!nvcc --version","metadata":{"_uuid":"7addc57e-1161-442b-a5d7-434a0f995b92","_cell_guid":"36f6ddb4-c076-4c98-9298-6de1986a260e","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -r /kaggle/working","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q torch\n!pip install -q git+https://github.com/huggingface/transformers #huggingface transformers for downloading models weights\n!pip install -U datasets #huggingface datasets to download and manipulate datasets\n!pip install -q peft #Parameter efficient finetuning - for qLora Finetuning\n!pip install -q bitsandbytes #For Model weights quantisation\n!pip install -q trl #Transformer Reinforcement Learning - For Finetuning using Supervised Fine-tuning\n!pip install -q wandb -U #Used to monitor the model score during training","metadata":{"_uuid":"96bbff70-e524-4e1f-9461-ae1164ad0ae0","_cell_guid":"57ee39ee-ee2c-48a4-b607-dcf572530bbb","collapsed":false,"id":"oNE24kyVq3L9","executionInfo":{"status":"ok","timestamp":1701624980310,"user_tz":-330,"elapsed":103798,"user":{"displayName":"Adarsh J","userId":"16994502924081621860"}},"outputId":"050212fb-ea96-4d57-f764-b68685bbcf97","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-06T12:25:21.610444Z","iopub.execute_input":"2024-01-06T12:25:21.610944Z","iopub.status.idle":"2024-01-06T12:27:14.781771Z","shell.execute_reply.started":"2024-01-06T12:25:21.610909Z","shell.execute_reply":"2024-01-06T12:27:14.780534Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nCollecting datasets\n  Obtaining dependency information for datasets from https://files.pythonhosted.org/packages/ec/93/454ada0d1b289a0f4a86ac88dbdeab54921becabac45da3da787d136628f/datasets-2.16.1-py3-none-any.whl.metadata\n  Downloading datasets-2.16.1-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.12.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.24.3)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (14.0.1)\nCollecting pyarrow-hotfix (from datasets)\n  Obtaining dependency information for pyarrow-hotfix from https://files.pythonhosted.org/packages/e4/f4/9ec2222f5f5f8ea04f66f184caafd991a39c8782e31f5b0266f101cb68ca/pyarrow_hotfix-0.6-py3-none-any.whl.metadata\n  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\nRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.0.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\nRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.10.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.5)\nRequirement already satisfied: huggingface-hub>=0.19.4 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.20.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (3.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nDownloading datasets-2.16.1-py3-none-any.whl (507 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\nInstalling collected packages: pyarrow-hotfix, datasets\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.1.0\n    Uninstalling datasets-2.1.0:\n      Successfully uninstalled datasets-2.1.0\nSuccessfully installed datasets-2.16.1 pyarrow-hotfix-0.6\n","output_type":"stream"}]},{"cell_type":"code","source":"#!pip3 install -q git+https://github.com/casper-hansen/AutoAWQ\n!pip3 install -q optimum\n!pip3 install -q auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/  # Use cu117 if on CUDA 11.7\n#!pip install auto-gptq","metadata":{"_uuid":"11d4a2f3-c2b1-4a8b-98df-582daea43c15","_cell_guid":"7a0cd815-a192-4bd7-afd3-7fcc9516a73c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-06T12:27:23.933575Z","iopub.execute_input":"2024-01-06T12:27:23.933973Z","iopub.status.idle":"2024-01-06T12:27:46.723391Z","shell.execute_reply.started":"2024-01-06T12:27:23.933940Z","shell.execute_reply":"2024-01-06T12:27:46.722093Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"pip install --upgrade huggingface_hub datasets","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip show huggingface_hub","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nimport re\nfrom pprint import pprint\n\nimport pandas as pd\nimport torch\nfrom datasets import Dataset, load_dataset\nfrom huggingface_hub import notebook_login\nfrom peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom trl import SFTTrainer # For supervised finetuning","metadata":{"_uuid":"24c1beca-4014-44a1-80b1-99ccb2bd1276","_cell_guid":"51d44599-9678-442f-ad1f-3182142e6bef","collapsed":false,"id":"2AQYegFzuCQG","executionInfo":{"status":"ok","timestamp":1701625012140,"user_tz":-330,"elapsed":28323,"user":{"displayName":"Adarsh J","userId":"16994502924081621860"}},"outputId":"a88c8abe-ff77-47fd-88b4-9c8facf9102f","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-06T12:28:03.460405Z","iopub.execute_input":"2024-01-06T12:28:03.461295Z","iopub.status.idle":"2024-01-06T12:28:23.513910Z","shell.execute_reply.started":"2024-01-06T12:28:03.461261Z","shell.execute_reply":"2024-01-06T12:28:23.513103Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"DOWN_DATA_PATH = 'AESLC/enron_subject_line'\nLINES_DATA_PATH = 'enron_lines'\n#MODEL_KEY = 'olm/olm-gpt2-dec-2022'\nMODEL_KEY = 'mistral-7B'\n#MODEL_KEY = 'gpt2'\nEXP_NAME = f'enron-subgen-{MODEL_KEY}'","metadata":{"_uuid":"22520f41-8260-43dc-aab6-e27e2b87f0ed","_cell_guid":"b665ce7b-2d95-43b7-aad4-cf3d855dc1a4","collapsed":false,"id":"1x0b_Z_4u-ah","executionInfo":{"status":"ok","timestamp":1701625016817,"user_tz":-330,"elapsed":381,"user":{"displayName":"Adarsh J","userId":"16994502924081621860"}},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-06T12:28:32.124728Z","iopub.execute_input":"2024-01-06T12:28:32.125325Z","iopub.status.idle":"2024-01-06T12:28:32.134070Z","shell.execute_reply.started":"2024-01-06T12:28:32.125278Z","shell.execute_reply":"2024-01-06T12:28:32.131649Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import random\nimport numpy as np\nimport os\nimport json\nSEED_VALUE = 15\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"","metadata":{"_uuid":"4697266e-1dc7-45ee-bb07-48bd045c009a","_cell_guid":"688eca3b-f299-4f80-80c8-accee66959c5","id":"9MVgkChtviFe","executionInfo":{"status":"ok","timestamp":1701625024712,"user_tz":-330,"elapsed":503,"user":{"displayName":"Adarsh J","userId":"16994502924081621860"}},"execution":{"iopub.status.busy":"2024-01-06T12:28:48.152994Z","iopub.execute_input":"2024-01-06T12:28:48.153361Z","iopub.status.idle":"2024-01-06T12:28:48.160155Z","shell.execute_reply.started":"2024-01-06T12:28:48.153335Z","shell.execute_reply":"2024-01-06T12:28:48.158886Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"random.seed(SEED_VALUE)\nnp.random.seed(SEED_VALUE)\ntorch.manual_seed(SEED_VALUE)","metadata":{"_uuid":"07824d27-fc19-4349-92be-6722d65d5329","_cell_guid":"67087010-041c-47c1-a921-1e3cbd5c3554","collapsed":false,"executionInfo":{"elapsed":406,"status":"ok","timestamp":1701625029363,"user":{"displayName":"Adarsh J","userId":"16994502924081621860"},"user_tz":-330},"id":"WdnqC3cXEyiO","outputId":"7127c21d-2d1a-4274-c16e-025ba9b7838c","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-06T12:29:03.922377Z","iopub.execute_input":"2024-01-06T12:29:03.922752Z","iopub.status.idle":"2024-01-06T12:29:03.938169Z","shell.execute_reply.started":"2024-01-06T12:29:03.922725Z","shell.execute_reply":"2024-01-06T12:29:03.935972Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7baed1735010>"},"metadata":{}}]},{"cell_type":"code","source":"def clean_text(text):\n    # Lowercase the text\n    #text = text.lower()\n    # Remove special characters\n    #text = re.sub(r'\\W', ' ', text)\n    # Remove extra white spaces\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text\n\ndef create_text_row(email, subject):\n    text_row = f\"\"\"<s>[INST] Generate a subject for this email content, {email} [/INST] \\\\n {subject} </s>\"\"\"\n    return text_row\n\ndef test_row(email):\n    text_row = f\"\"\"<s>[INST] Generate a subject for this email content, {email} [/INST] \"\"\"\n    return text_row","metadata":{"_uuid":"a5140715-6c51-4f4e-9c26-0abd1e008f2c","_cell_guid":"f003d8ec-bb93-4bb9-94b1-1df8e5d0d5a9","collapsed":false,"id":"fWgbLQ-1zgi0","executionInfo":{"status":"ok","timestamp":1701625032970,"user_tz":-330,"elapsed":476,"user":{"displayName":"Adarsh J","userId":"16994502924081621860"}},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-06T12:29:08.821728Z","iopub.execute_input":"2024-01-06T12:29:08.822263Z","iopub.status.idle":"2024-01-06T12:29:08.830825Z","shell.execute_reply.started":"2024-01-06T12:29:08.822224Z","shell.execute_reply":"2024-01-06T12:29:08.828985Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"os.makedirs(LINES_DATA_PATH,exist_ok=True)\ndef prepare_train_dataset(down_data_path,lines_data_path,split):\n    with open(os.path.join(lines_data_path,f'{split}.json'),'w') as writer:\n        filenames = os.listdir(os.path.join(down_data_path,split))\n        for filename in filenames:\n            with open(os.path.join(down_data_path,split,filename),'r') as reader:\n                email,subject = reader.read().split('@subject')\n                # email = ' '.join(email.replace('\\n',' ').split()).strip()\n                # subject = ' '.join(subject.replace('\\n',' ').split()).strip()\n\n                email = clean_text(email)\n                subject = clean_text(subject)\n\n                json_object = {\n                    \"text\": create_text_row(email, subject)\n                }\n               # sequence = '<email>' + email + '<subject>' + subject\n                writer.write(json.dumps(json_object) + \"\\n\")\n\n    return","metadata":{"_uuid":"d27ed5f6-62e9-4830-83ab-31185613606b","_cell_guid":"6abf6585-5e6d-4ca2-b6c2-7cae2f300356","collapsed":false,"id":"0t17xMZnyPkM","executionInfo":{"status":"ok","timestamp":1701625038286,"user_tz":-330,"elapsed":2,"user":{"displayName":"Adarsh J","userId":"16994502924081621860"}},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-06T12:29:25.183557Z","iopub.execute_input":"2024-01-06T12:29:25.183975Z","iopub.status.idle":"2024-01-06T12:29:25.192081Z","shell.execute_reply.started":"2024-01-06T12:29:25.183944Z","shell.execute_reply":"2024-01-06T12:29:25.190560Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def prepare_validation_dataset(down_data_path,lines_data_path,split):\n    with open(os.path.join(lines_data_path,f'{split}.json'),'w') as writer:\n        filenames = os.listdir(os.path.join(down_data_path,split))\n        for filename in filenames:\n            with open(os.path.join(down_data_path,split,filename),'r') as reader:\n                email,email_right = reader.read().split('@subject')\n                subject,subject_right  = email_right.split('@ann0')\n                ann0,ann0_right = subject_right.split('@ann1')\n                ann1,ann2 = ann0_right.split('@ann2')\n\n\n                # email = ' '.join(email.replace('\\n',' ').split()).strip()\n                # subject = ' '.join(subject.replace('\\n',' ').split()).strip()\n                # ann0 = ' '.join(ann0.replace('\\n',' ').split()).strip()\n                # ann1 = ' '.join(ann1.replace('\\n',' ').split()).strip()\n                # ann2 = ' '.join(ann2.replace('\\n',' ').split()).strip()\n\n                email = clean_text(email)\n                subject = clean_text(subject)\n                ann0 = clean_text(ann0)\n                ann1 = clean_text(ann1)\n                ann2 = clean_text(ann2)\n\n                json_object = {\n                    \"text\": test_row(email),\n                    \"subject\": subject,\n                    \"ann0\": ann0,\n                    \"ann1\": ann1,\n                    \"ann2\":ann2\n\n                }\n\n\n               # sequence = '<email>' + email + '<subject>' + subject + '<ann0>' + ann0 + '<ann1>' + ann1 + '<ann2>' + ann2\n                writer.write(json.dumps(json_object) + '\\n')\n\n    return","metadata":{"_uuid":"2f04af98-8a32-4627-bb0a-b308c284a585","_cell_guid":"4e5f10ce-2e9a-446f-9e69-9e2fc2895dfe","collapsed":false,"id":"GCG2ldl_IFrN","executionInfo":{"status":"ok","timestamp":1701625043663,"user_tz":-330,"elapsed":3,"user":{"displayName":"Adarsh J","userId":"16994502924081621860"}},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-06T12:29:28.926136Z","iopub.execute_input":"2024-01-06T12:29:28.926645Z","iopub.status.idle":"2024-01-06T12:29:28.936044Z","shell.execute_reply.started":"2024-01-06T12:29:28.926607Z","shell.execute_reply":"2024-01-06T12:29:28.934755Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"prepare_train_dataset(DOWN_DATA_PATH,LINES_DATA_PATH,'train')\nprepare_validation_dataset(DOWN_DATA_PATH,LINES_DATA_PATH,'dev')\n\nprepare_validation_dataset(DOWN_DATA_PATH,LINES_DATA_PATH,'test')","metadata":{"_uuid":"8e9c48f7-a82f-46db-915a-b6c0f6fb2967","_cell_guid":"456aab1c-5a17-4c3f-8da1-f34cc04f1fa3","collapsed":false,"id":"HM_UvJH_A3GA","executionInfo":{"status":"error","timestamp":1701625598931,"user_tz":-330,"elapsed":471008,"user":{"displayName":"Adarsh J","userId":"16994502924081621860"}},"outputId":"32591023-9183-4b1e-9d50-01ed64586955","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-06T12:29:36.367250Z","iopub.execute_input":"2024-01-06T12:29:36.367828Z","iopub.status.idle":"2024-01-06T12:29:37.886153Z","shell.execute_reply.started":"2024-01-06T12:29:36.367787Z","shell.execute_reply":"2024-01-06T12:29:37.884639Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# def load_dataset(file_path,tokenizer):\n#     block_size = 1024\n\n#     dataset = LineByLineTextDataset(\n#              tokenizer=tokenizer,\n#              file_path=file_path,\n#              block_size=block_size\n#     )\n#     return dataset","metadata":{"_uuid":"a05f45f5-6da4-43ee-8a17-92e65d44b16d","_cell_guid":"7cde2a83-a482-477e-acdb-ee1c0ee3a713","collapsed":false,"id":"2nfDQNNpI-ot","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_data_collator(tokenizer,mlm=False):\n    data_collator = DataCollatorForLanguageModeling(\n        tokenizer=tokenizer,\n        mlm=mlm\n    )\n    return data_collator","metadata":{"_uuid":"98da6983-7b98-4e96-ba97-a97e4d1ee091","_cell_guid":"9da26828-a6ca-45ae-9228-93c33931def7","collapsed":false,"id":"MLEFOBvyJCOu","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-06T12:29:44.733184Z","iopub.execute_input":"2024-01-06T12:29:44.733678Z","iopub.status.idle":"2024-01-06T12:29:44.741629Z","shell.execute_reply.started":"2024-01-06T12:29:44.733645Z","shell.execute_reply":"2024-01-06T12:29:44.739766Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"#from datasets import Dataset, load_dataset","metadata":{"_uuid":"ae76b5af-11a9-4ae1-8c57-73c2c72f9f50","_cell_guid":"c055d608-1a29-4ec7-a150-ca8255910f63","collapsed":false,"id":"Q2FRm8QOvD8E","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/input/mistral-7b","metadata":{"execution":{"iopub.status.busy":"2024-01-06T10:55:17.569323Z","iopub.execute_input":"2024-01-06T10:55:17.570288Z","iopub.status.idle":"2024-01-06T10:55:17.580217Z","shell.execute_reply.started":"2024-01-06T10:55:17.570251Z","shell.execute_reply":"2024-01-06T10:55:17.579181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = load_dataset('json', data_files='enron_lines/train.json', split='train')\ntest_dataset = load_dataset('json', data_files='enron_lines/test.json', split='train')\ndev_dataset = load_dataset('json',data_files='enron_lines/dev.json', split='train')","metadata":{"_uuid":"c30dac75-4b19-4242-afbb-c4c0f4cfc128","_cell_guid":"e8b0dba2-6b67-4864-8b76-8c8d35c29174","collapsed":false,"id":"vEE467h9ueOo","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-06T12:30:02.775607Z","iopub.execute_input":"2024-01-06T12:30:02.776068Z","iopub.status.idle":"2024-01-06T12:30:04.137842Z","shell.execute_reply.started":"2024-01-06T12:30:02.776035Z","shell.execute_reply":"2024-01-06T12:30:04.136846Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"184feca1adc4400cbb952004a916f12f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31f6ffab74d4497d8772e1f5d644ee0c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fc6586c27dd4706a2ae3d2decf291f5"}},"metadata":{}}]},{"cell_type":"code","source":"test_dataset","metadata":{"_uuid":"024301a5-85a8-4213-aab2-0a62d88c2d30","_cell_guid":"2dbf198b-450b-4e70-8500-9b1738ae0e24","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-06T12:34:47.690621Z","iopub.execute_input":"2024-01-06T12:34:47.691292Z","iopub.status.idle":"2024-01-06T12:34:47.699889Z","shell.execute_reply.started":"2024-01-06T12:34:47.691249Z","shell.execute_reply":"2024-01-06T12:34:47.698976Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['text', 'subject', 'ann0', 'ann1', 'ann2'],\n    num_rows: 1906\n})"},"metadata":{}}]},{"cell_type":"code","source":"model_name = \"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\"\n\n# Fine-tuned model name\nnew_model = \"mistralai-Email-Instruct\"","metadata":{"_uuid":"dcef4c7c-9ae2-497e-b75a-f6a30e7d8a31","_cell_guid":"0d407552-9262-4f9e-8723-904ead178f24","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-06T10:55:31.494142Z","iopub.execute_input":"2024-01-06T10:55:31.495162Z","iopub.status.idle":"2024-01-06T10:55:31.500765Z","shell.execute_reply.started":"2024-01-06T10:55:31.495126Z","shell.execute_reply":"2024-01-06T10:55:31.498539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"################################################################################\n# QLoRA parameters\n################################################################################\n\n# LoRA attention dimension\nlora_r = 64\n\n# Alpha parameter for LoRA scaling\nlora_alpha = 16\n\n# Dropout probability for LoRA layers\nlora_dropout = 0.1\n\n################################################################################\n# bitsandbytes parameters\n################################################################################\n\n# Activate 4-bit precision base model loading\nuse_4bit = True\n\n# Compute dtype for 4-bit base models\nbnb_4bit_compute_dtype = \"float16\"\n\n# Quantization type (fp4 or nf4)\nbnb_4bit_quant_type = \"nf4\"\n\n# Activate nested quantization for 4-bit base models (double quantization)\nuse_nested_quant = False\n\n################################################################################\n# TrainingArguments parameters\n################################################################################\n\n# Output directory where the model predictions and checkpoints will be stored\noutput_dir = \"./results\"\n\n# Number of training epochs\nnum_train_epochs = 1\n\n# Enable fp16/bf16 training (set bf16 to True with an A100)\nfp16 = False\nbf16 = False\n\n# Batch size per GPU for training\nper_device_train_batch_size = 4\n\n# Batch size per GPU for evaluation\nper_device_eval_batch_size = 4\n\n# Number of update steps to accumulate the gradients for\ngradient_accumulation_steps = 1\n\n# Enable gradient checkpointing\ngradient_checkpointing = True\n\n# Maximum gradient normal (gradient clipping)\nmax_grad_norm = 0.3\n\n# Initial learning rate (AdamW optimizer)\nlearning_rate = 2e-4\n\n# Weight decay to apply to all layers except bias/LayerNorm weights\nweight_decay = 0.001\n\n# Optimizer to use\noptim = \"paged_adamw_32bit\"\n\n# Learning rate schedule (constant a bit better than cosine)\nlr_scheduler_type = \"constant\"\n\n# Number of training steps (overrides num_train_epochs)\nmax_steps = -1\n\n# Ratio of steps for a linear warmup (from 0 to learning rate)\nwarmup_ratio = 0.03\n\n# Group sequences into batches with same length\n# Saves memory and speeds up training considerably\ngroup_by_length = True\n\n# Save checkpoint every X updates steps\nsave_steps = 100\n\n# Log every X updates steps\nlogging_steps = 100\n\n################################################################################\n# SFT parameters\n################################################################################\n\n# Maximum sequence length to use\nmax_seq_length = None\n\n# Pack multiple short examples in the same input sequence to increase efficiency\npacking = False\n\n# Load the entire model on the GPU 0\ndevice_map = {\"\": 0}","metadata":{"_uuid":"a48cb903-b72d-4ea6-98cc-eb17210bcf7c","_cell_guid":"dd64f441-abea-450b-81f3-9fcd83cd4bed","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-06T10:55:36.664437Z","iopub.execute_input":"2024-01-06T10:55:36.665384Z","iopub.status.idle":"2024-01-06T10:55:36.675059Z","shell.execute_reply.started":"2024-01-06T10:55:36.665346Z","shell.execute_reply":"2024-01-06T10:55:36.674030Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n\n# bnb_config = BitsAndBytesConfig(\n#     load_in_4bit=use_4bit,\n#     bnb_4bit_quant_type=bnb_4bit_quant_type,\n#     bnb_4bit_compute_dtype=compute_dtype,\n#     bnb_4bit_use_double_quant=use_nested_quant,\n# )","metadata":{"_uuid":"06ea830c-a13f-4520-b8a3-d30abaa5ff3c","_cell_guid":"96913f11-44c8-405e-84b4-bcdc1bced41a","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from awq import AutoAWQForCausalLM\nimport optimum","metadata":{"_uuid":"f995554a-2d5f-4376-aa28-11efd8c70d0e","_cell_guid":"54a4dca0-55dc-4e89-b10f-3de6ecdbd1cb","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-06T12:30:29.236998Z","iopub.execute_input":"2024-01-06T12:30:29.237556Z","iopub.status.idle":"2024-01-06T12:30:29.244834Z","shell.execute_reply.started":"2024-01-06T12:30:29.237502Z","shell.execute_reply":"2024-01-06T12:30:29.243379Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Load the base model with QLoRA configuration\n\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    #quantization_config=bnb_config,\n    device_map={\"\": 0},\n)\n\nbase_model = prepare_model_for_kbit_training(base_model)\n\nbase_model.config.use_cache = False\nbase_model.config.pretraining_tp = 1\nif torch.cuda.device_count() > 1: # If more than 1 GPU\n    base_model.is_parallelizable = True\n    base_model.model_parallel = True\n\n# Load MistralAI tokenizer","metadata":{"_uuid":"e0ae33ba-ae73-4fa5-9409-b5ef8c32ba05","_cell_guid":"6df5bae4-ddfc-4a3f-9650-49e69d905123","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-06T11:04:41.091723Z","iopub.execute_input":"2024-01-06T11:04:41.092107Z","iopub.status.idle":"2024-01-06T11:05:01.073828Z","shell.execute_reply.started":"2024-01-06T11:04:41.092076Z","shell.execute_reply":"2024-01-06T11:05:01.072745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"_uuid":"b0894f1a-7f96-40a9-a1cb-d5adee0e4184","_cell_guid":"940c3c1c-abc8-4146-94a9-e41529037f68","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-06T11:05:13.033753Z","iopub.execute_input":"2024-01-06T11:05:13.034151Z","iopub.status.idle":"2024-01-06T11:05:14.181144Z","shell.execute_reply.started":"2024-01-06T11:05:13.034120Z","shell.execute_reply":"2024-01-06T11:05:14.180100Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eval_prompt = \"\"\"Print hello world in python, C, and C++\"\"\"\n\nmodel_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n\nbase_model.eval()\nwith torch.no_grad():\n    print(tokenizer.decode(base_model.generate(**model_input, max_new_tokens=256, pad_token_id=2)[0], skip_special_tokens=True))","metadata":{"_uuid":"46e64195-2fcf-452e-84ab-dfb3bf6a4f23","_cell_guid":"72a009b5-e47f-4bdb-acd3-1e03f48b8217","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wandb\nwandb.login()\n#run = wandb.init(project=\"Email_subject_MIstral\")\nrun = wandb.init(project='Email_subject_MIstral', id='fstwjwem', resume=\"must\")","metadata":{"_uuid":"8b7958d8-1b60-414e-b96e-539251be01f9","_cell_guid":"9381cf61-c325-495d-b82f-2d3f5d81cb54","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-06T10:56:01.167490Z","iopub.execute_input":"2024-01-06T10:56:01.168258Z","iopub.status.idle":"2024-01-06T10:59:51.439614Z","shell.execute_reply.started":"2024-01-06T10:56:01.168203Z","shell.execute_reply":"2024-01-06T10:59:51.438706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install evaluate sacrebleu\n!pip install rouge_score","metadata":{"_uuid":"ef14ff15-c6f1-45a4-8a4f-0620b5a57289","_cell_guid":"2bd57d31-b917-402d-ae84-002b7b33bbba","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import evaluate","metadata":{"_uuid":"562d59af-abc0-48e7-a7b1-226bd068ce85","_cell_guid":"a52d0b39-eb5b-435e-b643-b3e2a781f801","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bleu = evaluate.load('bleu')\nrouge = evaluate.load('rouge')\n#meteor = evaluate.load('meteor')\n\n\ndef preprocess_logits_for_metrics(logits, labels):\n    pred_ids = torch.argmax(logits, dim=-1)\n\n    return pred_ids, labels\n\n\n\ndef compute_metrics(eval_preds):\n    logits, labels = eval_preds\n    preds = logits[0]\n    preds = np.where(preds!= -100,preds, tokenizer.pad_token_id)\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    sequences = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    prompts = list()\n    references = list()\n    for sequence in sequences:\n        try:\n            prompt = sequence[\"text\"]\n            subject =sequence[\"subject\"]\n            ann0 = sequence[\"ann0\"]\n            ann1 = sequence[\"ann1\"]\n            ann2 = sequence[\"ann2\"]\n            prompts.append(prompt)\n            references.append([subject, ann0, ann1, ann2])\n        except ValueError:\n            continue\n\n    tokenizer.padding_side='left'\n    n_batches = math.ceil(len(prompts)*1.0 / per_device_eval_batch_size)\n    outputs = list()\n    for i in range(n_batches):\n        prompts_batch = prompts[i*per_device_eval_batch_size : (i+1)*per_device_eval_batch_size]\n        prompts_batch_ids = tokenizer(prompts_batch,\n            padding=True, truncation=True, return_tensors='pt').to(\"cuda\")\n        output_ids = base_model.generate(\n            **prompts_batch_ids, max_new_tokens=10,\n            pad_token_id=tokenizer.pad_token_id)\n        outputs_batch = [seq.split('<subject>')[1] for seq in\n            tokenizer.batch_decode(output_ids, skip_special_tokens=True)]\n        outputs.extend(outputs_batch)\n    tokenizer.padding_side='right'\n\n    bleu_score = bleu.compute(predictions=outputs, references=references)\n    rouge_score = rouge.compute(predictions=outputs, references=references)\n   # meteor_score = meteor.compute(predictions=outputs, references=references)\n\n    return {\n        'BLEU': round(bleu_score['bleu'], 4) * 100,\n        'R1': round(rouge_score['rouge1'], 4) * 100,\n        'R2': round(rouge_score['rouge2'], 4) * 100,\n        'RL': round(rouge_score['rougeL'], 4) * 100,\n        'RLsum': round(rouge_score['rougeLsum'], 4) * 100,\n        #'meteor': round(meteor_score['meteor'], 4) * 100,\n\n        #'METEOR': round(meteor_score['meteor'], 4) * 100\n        }","metadata":{"_uuid":"a7de267f-e1fc-4844-b616-4e929c880a3b","_cell_guid":"d3233eee-0734-41aa-bd70-3c3ccf9e6e81","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run.id","metadata":{"_uuid":"d808cfb7-4658-448d-8d32-eb1690965cbf","_cell_guid":"0e67902a-b36c-4a60-96ff-a4509cac253d","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%ls /kaggle/working/artifacts","metadata":{"execution":{"iopub.status.busy":"2024-01-06T11:00:15.563213Z","iopub.execute_input":"2024-01-06T11:00:15.563624Z","iopub.status.idle":"2024-01-06T11:00:16.527628Z","shell.execute_reply.started":"2024-01-06T11:00:15.563596Z","shell.execute_reply":"2024-01-06T11:00:16.526244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -r ./artifacts","metadata":{"execution":{"iopub.status.busy":"2024-01-06T11:00:26.440029Z","iopub.execute_input":"2024-01-06T11:00:26.440826Z","iopub.status.idle":"2024-01-06T11:00:27.401643Z","shell.execute_reply.started":"2024-01-06T11:00:26.440789Z","shell.execute_reply":"2024-01-06T11:00:27.400461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2024-01-06T11:00:37.626420Z","iopub.execute_input":"2024-01-06T11:00:37.626814Z","iopub.status.idle":"2024-01-06T11:00:37.636923Z","shell.execute_reply.started":"2024-01-06T11:00:37.626784Z","shell.execute_reply":"2024-01-06T11:00:37.635797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -r ./results","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"last_run_id = 'fstwjwem'\nmy_checkpoint_name = f\"checkpoint-{last_run_id}:v37\"\nmy_checkpoint_artifact = run.use_artifact(my_checkpoint_name)\ncheckpoint_dir = my_checkpoint_artifact.download()\n#base_model = AutoModelForCausalLM.from_pretrained('/kaggle/working/artifacts/checkpoint-jxdoo6gc:v6', quantization_config=bnb_config,device_map={\"\": 0})","metadata":{"_uuid":"207152be-8b7c-49b9-b226-9ee60e499856","_cell_guid":"f4bb5798-8bc8-4e4e-aa53-4b1750a2bbb9","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-06T11:03:13.762107Z","iopub.execute_input":"2024-01-06T11:03:13.763102Z","iopub.status.idle":"2024-01-06T11:03:16.977622Z","shell.execute_reply.started":"2024-01-06T11:03:13.763064Z","shell.execute_reply":"2024-01-06T11:03:16.976474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load LoRA configuration\n\nos.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"\npeft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_r,\n    target_modules=[\n        \"q_proj\",\n        \"k_proj\",\n        \"v_proj\",\n        \"o_proj\",\n        \"gate_proj\",\n        \"up_proj\",\n        \"down_proj\",\n        \"lm_head\",\n    ],\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\n\n\n\n\n# Set training parameters\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=num_train_epochs,\n    per_device_train_batch_size=per_device_train_batch_size,\n    per_device_eval_batch_size=per_device_eval_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    save_steps=save_steps,\n    eval_steps = 100,\n    logging_steps=logging_steps,\n    learning_rate=learning_rate,\n    weight_decay=weight_decay,\n    fp16=fp16,\n    bf16=bf16,\n    max_grad_norm=max_grad_norm,\n    max_steps=1000, # the number of training steps the model will take\n    warmup_ratio=warmup_ratio,\n    group_by_length=group_by_length,\n    lr_scheduler_type=lr_scheduler_type,\n    report_to=\"wandb\"\n)\n\n# Set supervised fine-tuning parameters\ntrainer = SFTTrainer(\n    model=base_model,\n    train_dataset=train_dataset,\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing=packing,\n    eval_dataset=dev_dataset,\n#     preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n#     compute_metrics=compute_metrics\n)","metadata":{"_uuid":"4f1e3a89-dd24-4384-954f-8df26800c8ea","_cell_guid":"ef595594-678a-4c34-979c-6e5e34aa2944","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-06T11:05:28.306962Z","iopub.execute_input":"2024-01-06T11:05:28.307938Z","iopub.status.idle":"2024-01-06T11:05:35.291116Z","shell.execute_reply.started":"2024-01-06T11:05:28.307902Z","shell.execute_reply":"2024-01-06T11:05:35.290130Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#trainer.train()\n\ntrainer.train(resume_from_checkpoint='/kaggle/working/artifacts/checkpoint-fstwjwem:v37')\n\n# Save trained model\ntrainer.model.save_pretrained(new_model)","metadata":{"_uuid":"3fead8b8-5c2b-4e9e-8156-bef4b1ad6dd1","_cell_guid":"36f6288e-3074-451d-9f46-9fff1fc9ae58","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-06T11:05:50.715891Z","iopub.execute_input":"2024-01-06T11:05:50.716683Z","iopub.status.idle":"2024-01-06T11:15:15.863687Z","shell.execute_reply.started":"2024-01-06T11:05:50.716647Z","shell.execute_reply":"2024-01-06T11:15:15.862563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -r ./results","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\n#del base_model\ngc.collect()\n\ndel trainer\ngc.collect()","metadata":{"_uuid":"40cc05cd-6562-4955-85b5-dd09f102a3bc","_cell_guid":"10ffbe95-799e-4886-81ad-e0544a79fb48","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache() # PyTorch thing","metadata":{"_uuid":"d92fc46d-7d8e-4c70-a4e4-a92f60f3a757","_cell_guid":"6545eb2e-fd61-428d-a95e-21bc8c67248d","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"_uuid":"1253f7ca-c822-4035-b07d-773846019a1d","_cell_guid":"c9ab2df5-ce6d-4f3f-b4d8-22ae2c269946","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    #low_cpu_mem_usage=True,\n    return_dict=True,\n    torch_dtype=torch.float16,\n    device_map={\"\": 0},\n)","metadata":{"_uuid":"c230c56b-d730-4ca0-a619-312179cfaed8","_cell_guid":"76c9904f-f3f3-409e-9e39-46bfdfee773e","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nmerged_model= PeftModel.from_pretrained(base_model, new_model)\nmerged_model= merged_model.merge_and_unload()\n\n# Save the merged model\nmerged_model.save_pretrained(\"merged_model\",safe_serialization=True)\ntokenizer.save_pretrained(\"merged_model\")\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"_uuid":"8e2b0639-ce5c-4d91-9a5a-25067b7e2c2b","_cell_guid":"46c42eea-8b3d-43d5-8e07-13901b95f493","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt = \"<s>[INST] Generate a subject for this email content, Thanks in advance for agreeing to speak at the Global Operations Controller Forum. There will be approximately 30 Enron business controllers present at the meeting. All have responsibility for mid and back office operations for the following Enron entities: Enron North America, Enron Europe, Enron South America, Enron Global Markets, Enron Industrial Markets, Enron Broadband Services and Enron Energy Services. Attendees will be here from Houston, Calgary, Tokyo, Sydney, London and New York (metals business). Attached for your reference is the agenda. There may be some slight changes before the forum begins, but this will give you a good idea of the topics to be covered and the other speakers who will address the group. You are scheduled to address the group as follows: [/INST] \"\n\ninput_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=False).input_ids.cuda()\n\noutputs = merged_model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=True, top_p=0.9,temperature=0.5)","metadata":{"_uuid":"faa37424-5f66-41c2-8ce4-d7d468a8d8fc","_cell_guid":"61f36c82-7fcc-4d91-a24b-9213c4e4f1c0","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Prompt:\\n{prompt}\\n\")\nprint(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True))","metadata":{"_uuid":"d08e313b-5813-4799-af60-354fa3eccfd2","_cell_guid":"447347c8-58ff-4447-8214-87ae47116414","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"87684a9f-1e48-4c53-958e-706bfce6e566","_cell_guid":"306c18eb-856b-4751-bb3c-287424084ed0","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Testing purposes, separately executed**","metadata":{"_uuid":"731bdc31-d305-456e-872e-a025fd57001f","_cell_guid":"eee95ce2-714a-4556-8eb8-ea5253bf1b65","trusted":true}},{"cell_type":"code","source":"def test_data():\n    prompts = list()\n    references = list()\n    with open(\"/kaggle/input/mistral-7b/enron_lines/test.json\", \"r\") as sequences:\n        for sequence in sequences:\n            sequence = json.loads(sequence)\n            try:\n                prompt = sequence[\"text\"]\n                subject =sequence[\"subject\"]\n                ann0 = sequence[\"ann0\"]\n                ann1 = sequence[\"ann1\"]\n                ann2 = sequence[\"ann2\"]\n                prompts.append(prompt)\n                references.append([subject, ann0, ann1, ann2])\n            except ValueError:\n                continue\n\n    tokenizer.padding_side='left'\n  #  n_batches = math.ceil(len(prompts)*1.0 / per_device_eval_batch_size)\n    outputs = list()\n#     for i in range(n_batches):\n#         prompts_batch = prompts[i*per_device_eval_batch_size : (i+1)*per_device_eval_batch_size]\n#         prompts_batch_ids = tokenizer(prompts_batch,\n#             padding=True, truncation=True, return_tensors='pt').to(\"cuda\")\n#         output_ids = base_model.generate(\n#             **prompts_batch_ids, max_new_tokens=10,\n#             pad_token_id=tokenizer.pad_token_id)\n#         outputs_batch = [seq[0][len(prompt):] for seq in\n#             tokenizer.batch_decode(output_ids, skip_special_tokens=True)]\n#         outputs.extend(outputs_batch)\n    for prompt in prompts:\n        input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n        output_ids = merged_model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=True, top_p=0.9,temperature=0.5)\n        output = tokenizer.batch_decode(output_ids.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]\n        print(output)\n        outputs.extend(output)\n    \n    \n        \n        \n    tokenizer.padding_side='right'\n    \n    df = pd.DataFrame({\n        'prompt':prompts,\n        'references':references,\n        'model_outputs':outputs\n    })\n\n    df.to_csv('test_output.csv', index=False)\n\n\n    bleu_score = bleu.compute(predictions=outputs, references=references)\n    rouge_score = rouge.compute(predictions=outputs, references=references)\n   # meteor_score = meteor.compute(predictions=outputs, references=references)\n\n    return {\n        'BLEU': round(bleu_score['bleu'], 4) * 100,\n        'R1': round(rouge_score['rouge1'], 4) * 100,\n        'R2': round(rouge_score['rouge2'], 4) * 100,\n        'RL': round(rouge_score['rougeL'], 4) * 100,\n        'RLsum': round(rouge_score['rougeLsum'], 4) * 100,\n        #'meteor': round(meteor_score['meteor'], 4) * 100,\n\n        #'METEOR': round(meteor_score['meteor'], 4) * 100\n        }","metadata":{"_uuid":"9c6f66e2-b13e-4f3b-a053-a6b34c9777b9","_cell_guid":"db8bf290-74e7-4537-8a05-a68f406fea68","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data()","metadata":{"_uuid":"d155bdf4-de36-4c04-aede-f9abb4f5bed9","_cell_guid":"ec7539f6-d50c-480d-b6f1-6a6356f4850f","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"0df5fc92-6eda-4b2c-b437-5f5a21a4ef46","_cell_guid":"e5fe2cc9-7bd5-4fb5-882a-9a522420c955","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"c620e9bd-4624-42a4-9068-7cce0ae5ad73","_cell_guid":"da19d18e-77be-4761-b9d6-352dd628cf99","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}